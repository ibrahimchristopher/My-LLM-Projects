{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7811935,"sourceType":"datasetVersion","datasetId":4575642},{"sourceId":4298,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3093}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#in this notebook we are going to leverage LLMS in automatic CV Curation\n\n#workflow in this notebook s as follows\n##Set up an LLM\n##Load up a resume\n##Using A singular Description, augment the resumee\n##save back as a pdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-11T11:47:33.199692Z","iopub.execute_input":"2024-03-11T11:47:33.200088Z","iopub.status.idle":"2024-03-11T11:47:33.205595Z","shell.execute_reply.started":"2024-03-11T11:47:33.200059Z","shell.execute_reply":"2024-03-11T11:47:33.204583Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#filepath to resumee\ncv_filepath = \"/kaggle/input/resume/Ibrahim_Ibrahim_Resume Febuary.pdf\"","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:47:33.210580Z","iopub.execute_input":"2024-03-11T11:47:33.210862Z","iopub.status.idle":"2024-03-11T11:47:33.221378Z","shell.execute_reply.started":"2024-03-11T11:47:33.210840Z","shell.execute_reply":"2024-03-11T11:47:33.220497Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#imports to set up LLMS\n\n!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain xformers==0.0.21 bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12\nfrom transformers import BitsAndBytesConfig, AutoConfig, AutoTokenizer, pipeline, AutoModelForCausalLM\nfrom torch import cuda, bfloat16\nimport torch\nfrom time import time\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:47:33.233252Z","iopub.execute_input":"2024-03-11T11:47:33.233733Z","iopub.status.idle":"2024-03-11T11:51:14.947457Z","shell.execute_reply.started":"2024-03-11T11:47:33.233709Z","shell.execute_reply":"2024-03-11T11:51:14.946435Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting transformers==4.33.0\n  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.22.0\n  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting langchain\n  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\nCollecting xformers==0.0.21\n  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\nCollecting sentence_transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting chromadb==0.4.12\n  Downloading chromadb-0.4.12-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.1.2)\nCollecting torch>=1.10.0 (from accelerate==0.22.0)\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.16.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.2.0)\nCollecting pydantic<2.0,>=1.9 (from chromadb==0.4.12)\n  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.2/150.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nCollecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.12)\n  Downloading fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.12)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb==0.4.12)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nCollecting pypika>=0.48.9 (from chromadb==0.4.12)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (6.1.1)\nCollecting bcrypt>=4.0.1 (from chromadb==0.4.12)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.12) (0.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch>=1.10.0->accelerate==0.22.0)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.42.0)\nCollecting cmake (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\nCollecting lit (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0)\n  Downloading lit-17.0.6.tar.gz (153 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.25 (from langchain)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\nCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.23-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nCollecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12)\n  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.2.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.2.0)\nCollecting packaging>=20.0 (from transformers==4.33.0)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (3.20.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.12)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: backoff>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.8.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2024.2.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.33.0) (1.26.18)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (12.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (9.5.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\nDownloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chromadb-0.4.12-py3-none-any.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.5/426.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.1.11-py3-none-any.whl (807 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cmake-3.28.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sentence_transformers, pypika, lit\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=e12a626eabfb97afc143cc8f34e6337594586370b40410242f9f81afc6780edd\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=b6056d3004953e2a61efe254b9b82028f42123672dc3b1ffb7b50dc39aa0fa04\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=5992cabaa86371e62f5082edc8ae0c79c882e83165b2d687040b476be28b8269\n  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\nSuccessfully built sentence_transformers pypika lit\nInstalling collected packages: tokenizers, pypika, monotonic, lit, cmake, bitsandbytes, pydantic, pulsar-client, packaging, orjson, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, humanfriendly, einops, chroma-hnswlib, bcrypt, starlette, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, transformers, onnxruntime, langchain-core, fastapi, langchain-text-splitters, langchain-community, chromadb, langchain, triton, torch, xformers, sentence_transformers, accelerate\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.32.0.post1\n    Uninstalling starlette-0.32.0.post1:\n      Successfully uninstalled starlette-0.32.0.post1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.108.0\n    Uninstalling fastapi-0.108.0:\n      Successfully uninstalled fastapi-0.108.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.27.2\n    Uninstalling accelerate-0.27.2:\n      Successfully uninstalled accelerate-0.27.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.33.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.22.0 bcrypt-4.1.2 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.28.3 coloredlogs-15.0.1 einops-0.6.1 fastapi-0.99.1 humanfriendly-10.0 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.23 lit-17.0.6 monotonic-1.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.17.1 orjson-3.9.15 packaging-23.2 posthog-3.5.0 pulsar-client-3.4.0 pydantic-1.10.14 pypika-0.48.9 sentence_transformers-2.2.2 starlette-0.27.0 tokenizers-0.13.3 torch-2.0.1 transformers-4.33.0 triton-2.0.0 xformers-0.0.21\n","output_type":"stream"},{"name":"stderr","text":"2024-03-11 11:50:59.932108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-11 11:50:59.932220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-11 11:51:00.070007: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"##USING THE LLAMA 7 billion parameters\n\nmodel_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\nmodel_config = AutoConfig.from_pretrained(\n    model_id,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_1 = time()\nquery_pipeline = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:51:14.949136Z","iopub.execute_input":"2024-03-11T11:51:14.949712Z","iopub.status.idle":"2024-03-11T11:54:08.713677Z","shell.execute_reply.started":"2024-03-11T11:51:14.949685Z","shell.execute_reply":"2024-03-11T11:54:08.712737Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73b517121c74935830eb0354535d4de"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare pipeline: 0.0 sec.\n","output_type":"stream"}]},{"cell_type":"code","source":"##SETTING UP PIPELINE AND TESTING RESUMEE\nllm = HuggingFacePipeline(pipeline=query_pipeline)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:41:46.129884Z","iopub.execute_input":"2024-03-11T04:41:46.130174Z","iopub.status.idle":"2024-03-11T04:41:46.136018Z","shell.execute_reply.started":"2024-03-11T04:41:46.130150Z","shell.execute_reply":"2024-03-11T04:41:46.135120Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"time_1 = time()\nresult = llm(prompt=\"Produce a standard data science resumee, fill it up with imaginary roles, skills, etc. formatting should be considerd\")\ntime_2 = time()\nprint(f\"Prepare Solution: {round(time_2-time_1, 3)} sec.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T04:41:46.137866Z","iopub.execute_input":"2024-03-11T04:41:46.138145Z","iopub.status.idle":"2024-03-11T04:42:45.202040Z","shell.execute_reply.started":"2024-03-11T04:41:46.138121Z","shell.execute_reply":"2024-03-11T04:42:45.201030Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare Solution: 59.05 sec.\n as well.\n\nHere is an example of a fictional data science resume:\n\n[Your Name]\n[Address]\n[City, State Zip]\n[Phone Number]\n[Email Address]\n\nObjective:\nTo leverage my skills in data analysis, machine learning, and visualization to drive business growth and improve decision-making processes.\n\nSummary:\nHighly motivated and detail-oriented data scientist with 5 years of experience in data analysis, machine learning, and data visualization. Skilled in a variety of programming languages, including Python, R, and SQL. Proficient in working with large datasets and experience with data visualization tools such as Tableau and Power BI.\n\nEducation:\n\n* Master of Science in Data Science, [University Name], [Graduation Date]\n\t+ Coursework: Data Mining, Machine Learning, Data Visualization, Statistical Computing, and Data Communication\n* Bachelor of Science in Computer Science, [University Name], [Graduation Date]\n\t+ Coursework: Data Structures, Algorithms, Computer Systems, and Programming Languages\n\nSkills:\n\n* Data Analysis: Python, R, SQL, Excel\n* Machine Learning: Python, R, TensorFlow, Keras\n* Data Visualization: Tableau, Power BI, D3.js\n* Programming Languages: Python, R, Java, JavaScript\n* Data Storage: MySQL, PostgreSQL, MongoDB\n* Data Wrangling: Pandas, NumPy, Scikit-learn\n* Data Engineering: AWS, Azure, GCP\n\nExperience:\n\nData Scientist, [Company Name], [Employment Dates]\n\t+ Worked with cross-functional teams to develop and deploy machine learning models for predictive maintenance and quality control\n\t+ Designed and implemented data visualizations to communicate insights to stakeholders\n\t+ Developed and maintained data pipelines using AWS and Python\n\t+ Collaborated with data engineers to optimize data storage and retrieval\n\nData Analyst, [Company Name], [Employment Dates]\n\t+ Analyzed customer data to identify trends and opportunities for growth\n\t+ Created dashboards and reports using Tableau to communicate insights to stakeholders\n\t+ Worked with the marketing team to develop targeted campaigns based on customer segmentation\n\t+ Developed and maintained SQL queries to extract insights from large datasets\n\nProjects:\n\n* Developed a predictive model for customer churn using machine learning techniques and Python\n* Built a data visualization dashboard using Tableau to display real-time sales data\n* Created a recommendation system using collaborative filtering and R\n* Analyzed customer sentiment using natural language processing and Python\n\nCertifications:\n\n* Certified Data Scientist, Data Science Council of America, [Certification Date]\n* Certified Analytics Professional, Institute for Operations Research and the Management Sciences, [Certification Date]\n\nReferences:\nAvailable upon request.\n\nNote: This is just an example resume, and you should customize it to fit your own experience and qualifications. Also, be sure to proofread your resume carefully to ensure there are no errors or typos.\n","output_type":"stream"}]},{"cell_type":"code","source":"##LOADING UP RESUMEE AS TEXT\n\npip install pypdf","metadata":{"execution":{"iopub.status.busy":"2024-03-11T05:04:46.919568Z","iopub.execute_input":"2024-03-11T05:04:46.919951Z","iopub.status.idle":"2024-03-11T05:04:59.430017Z","shell.execute_reply.started":"2024-03-11T05:04:46.919921Z","shell.execute_reply":"2024-03-11T05:04:59.428767Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (4.0.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(cv_filepath)\npages = loader.load_and_split()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:59:47.418572Z","iopub.execute_input":"2024-03-11T11:59:47.418972Z","iopub.status.idle":"2024-03-11T11:59:47.531711Z","shell.execute_reply.started":"2024-03-11T11:59:47.418945Z","shell.execute_reply":"2024-03-11T11:59:47.530945Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"all_pages = ''.join([page.page_content for page in pages])","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:59:49.396169Z","iopub.execute_input":"2024-03-11T11:59:49.396835Z","iopub.status.idle":"2024-03-11T11:59:49.401327Z","shell.execute_reply.started":"2024-03-11T11:59:49.396803Z","shell.execute_reply":"2024-03-11T11:59:49.400319Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(all_pages)\n\n##RESUMEE LOADED","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:59:50.456842Z","iopub.execute_input":"2024-03-11T11:59:50.457511Z","iopub.status.idle":"2024-03-11T11:59:50.462135Z","shell.execute_reply.started":"2024-03-11T11:59:50.457475Z","shell.execute_reply":"2024-03-11T11:59:50.461225Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Ibrahim Ibrahim\n♂phone+234-815-448-1095 /envel⌢peibrahimchristopher97@gmail.com /linkedinlinkedin /githubgithub\nEducation and Honours\nUniversity of Lagos Sep. 2016 – July 2021\nBachelor of Science in Chemical Engineering Akoka, Lagos\nRelevant Coursework\n•Process Optimization\n•Cybersecurity•Probability and Statistics\n•Engineering Analysis•Numerical Methods\n•Linear Algebra•Process Control\n•Calculus\nScholarships And Research Grants\n•Petroluem Technology Development Fund Scholarship For Academic Excellence [Top 300 Student Nationwide]\n•Total E&P Scholarship [For Outstanding Students in the Country]\n•Nigerian Agip oil Company Scholarship Award [For Top Performing Student per State]\nExperience\n10Alytics January 2024 – Present\nSenior Data Associate Manchester, United Kingdom\n•Spearheaded tailored training sessions, empowering individuals and organizations with essential data management skills,\nincluding Excel data cleaning, dashboarding, and statistical analysis.\n•Explored emerging technologies such as Large Language Models (LLMs), Computer Vision (CV), and Natural Language\nProcessing (NLP) in research and product design, implementing innovative solutions across diverse domains.\n•Facilitated engaging training experiences, fostering a culture of continuous learning and growth, and successfully training\nover 50,000 individuals in essential data science concepts and tools.\nAmdari September 2023 – February 2024\nSubject Matter Expert, Data Science Manchester, United Kingdom\n•Led the successful conclusion of multiple impactful case studies across energy, health, music, and finance sectors.\n•Applied cutting-edge techniques like computer vision, large language models, and traditional machine learning to tackle\ndiverse business challenges.\n•Developed innovative solutions using a robust stack including RabbitMQ, Snowflake Data Warehouse, Flask, and Cloud\nInstances for seamless deployment.\n•Completed 14 crucial projects in just 6 months, guiding each through experimentation, modeling, and deployment phases\nwith precision.\nJosplay Inc.(An African Music Intelligence Startup) June 2022 – August 2023\nJunior Machine Learning Engineer Lagos, Lagos\n•Spearheaded the successful development and deployment of cutting-edge deep learning models for audio analysis,\ninstrument detection, song genre classification, and music recommendation. Utilized state-of-the-art tools and\nframeworks such as TensorFlow and Pytorch for development and containerization technologies like Docker to deliver\nexceptional performance and accuracy.\n•Pioneered the use of advanced audio signal processing libraries such as Librosa and Essentia to create robust and scalable\npipelines for audio processing, analysis, and synthesis.\n•Engineered and implemented high-performance, fault-tolerant systems that have enabled the real-time analysis of over\n1,000,000 songs, delivering fast and accurate results at scale. Leveraged the power of leading cloud service providers like\nAmazon Web Services(AWS) and Google Cloud Platform(GCP), as well as technologies such as Pymongo (MongoDB),\nFlask, and Kafka, to build robust and reliable pipelines for audio processing and analysis.\nNational Information Technology Development Agency(NITHUB) April 2021 – April 2022\nData Science Research Team Lead at NITHUB Unilag Lagos, Lagos\n•Led a team of Data Scientists in conducting groundbreaking research aimed at improving traffic management systems\nthrough the application of advanced computer vision technique\n•Developed state-of-the-art models with exceptional accuracy for real-time traffic pattern detection and analysis,\nleveraging powerful tools such as TensorFlow, Keras, and a pretrained YOLO v5 model.\n•Developed a cutting-edge traffic lighting system that dynamically adapts to traffic patterns, resulting in a significant 15%\nreduction in wait time for motorists.\nUniversity Of Lagos November 2020 – May 2021\nData Research Assistant Lagos, Lagos•Worked On Chemical Dehydration Research, Analyzed Dehydration Experiment data in a bid to improve Industry\nProcess and Equipment Efficiency\n•Used Matlab, Sci-Lab, Python and Excel to Create And Compare viable Dehydration Models best fitted to experimental\ndata\n•Employed Artificial Neural Networks, Other Various Deep Learning Modelling Techniques to better model experimental\nData to predict Moisture ratio of Agricultural Produce During Dehydration\nRoyal Salt Limited May 2020 – October 2020\nData Analyst Intern Lagos, Lagos\n•Assisted in analysing product quality data and detecting anomalies in product quality using Excel\n•Collaborated with team members to improve faulty product detection rate by 27.5%\nProjects\nRAG-LLM System For Ingesting and Querying Companys’ Sustainability reports Using LLAMA And Langchain\n•Leveraging Langchain and Open Source, Large Language Model: LLAMA 7B, i was able to create a pipeline for the\nIngestion, Tokenization and Querying of Information avalaible in Company’s Sustainbilty Report\nResearch\nAPPLICATIONS OF ARTIFICIAL NEURAL NETWORKS IN MODELLING THE DE-\nHYDRATION PROPERTIES OF YAM(DIOSCOREA ROTUNDA)|Publication in Review\n•Authors: Dr. AA Akinola (Google Scholar), Ibrahim Ibrahim, Obafemi Dolapo\nTechnical Skills\nLanguages : Python, Java, Solidity, HTML/CSS, JavaScript, SQL, Matlab\nDeveloper Tools : VS Code, AWS CLI, Google Colaboratory, Google Cloud Platform\nTechnologies/Frameworks : Linux, Tensorflow, Pytorch, Scikit-Learn, Pandas, Numpy, Seaborn, Pyplot, Hyperopt,\nKafka, RabbitMQ, SnowFlakes, Langchain, Vader\nLeadership / Extracurricular\nGoogle Student Developer’s Club Open Hack Week 2020 July, 2020\nTeam Lead University of Lagos\n•Successfully led a team of 5 in developing a groundbreaking online platform that enables the exchange of recyclable waste\nfor cash, with the ultimate goal of building a zero waste world. Conducted a thorough viability study of the product and\ncrafted a highly effective pitch deck that propelled us to victory in the finals of the challenge.\n","output_type":"stream"}]},{"cell_type":"code","source":"##Now i take a typical job description from the web abd save as a var caled resp\nresp = \"\"\"Key Responsibilities\nAnalyze geospatial data using statistical and machine learning techniques.\nDevelop spatial analytics models and algorithms for applications such as location intelligence, urban planning, and environmental monitoring.\nVisualize and communicate spatial analysis results through maps, charts, and reports.\nCollaborate with domain experts to define spatial analysis requirements and objectives.\nStay updated with advancements in spatial data science, GIS tools, and remote sensing technologies.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:04:53.261822Z","iopub.execute_input":"2024-03-11T12:04:53.262242Z","iopub.status.idle":"2024-03-11T12:04:53.267157Z","shell.execute_reply.started":"2024-03-11T12:04:53.262205Z","shell.execute_reply":"2024-03-11T12:04:53.266037Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(resp)\n#noting the number of characters in this job desc\nlen(f\"i want you to edit my resume to be better compatible with the job description for a job opening, this is my reumee: {all_pages}, this is the job responsibilities: {resp}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T05:30:39.699551Z","iopub.execute_input":"2024-03-11T05:30:39.699906Z","iopub.status.idle":"2024-03-11T05:30:39.706854Z","shell.execute_reply.started":"2024-03-11T05:30:39.699880Z","shell.execute_reply":"2024-03-11T05:30:39.705828Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Key Responsibilities\nAnalyze geospatial data using statistical and machine learning techniques.\nDevelop spatial analytics models and algorithms for applications such as location intelligence, urban planning, and environmental monitoring.\nVisualize and communicate spatial analysis results through maps, charts, and reports.\nCollaborate with domain experts to define spatial analysis requirements and objectives.\nStay updated with advancements in spatial data science, GIS tools, and remote sensing technologies.\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"6618"},"metadata":{}}]},{"cell_type":"code","source":"##now lets prompt our llm to augment the resumee to fit the job desc\nresult = llm(prompt=f\"i want you to edit my resume to be better compatible with the job description for a job opening, this is my resumee: {all_pages}, this is the job responsibilities: {resp}.. return ONLY the edited resume, no other acompaning sentences. HIGHLIGHT THE CHANGES IN UPPER CASE\")","metadata":{"execution":{"iopub.status.busy":"2024-03-11T05:30:40.013961Z","iopub.execute_input":"2024-03-11T05:30:40.014316Z","iopub.status.idle":"2024-03-11T05:32:41.222822Z","shell.execute_reply.started":"2024-03-11T05:30:40.014288Z","shell.execute_reply":"2024-03-11T05:32:41.221769Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T05:32:41.224430Z","iopub.execute_input":"2024-03-11T05:32:41.224709Z","iopub.status.idle":"2024-03-11T05:32:41.229735Z","shell.execute_reply.started":"2024-03-11T05:32:41.224686Z","shell.execute_reply":"2024-03-11T05:32:41.228858Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":".\n\nI edited your resume to make it more compatible with the job description you provided. Here are the changes I made:\n\nEDUCATION AND HONOURS\n\n* University of Lagos (September 2020 – July 2021)\n\t+ Bachelor of Science in Chemical Engineering\n\t+ Relevant Coursework: Process Optimization, Cybersecurity, Probability and Statistics, Engineering Analysis, Numerical Methods, Linear Algebra, Process Control, Calculus\n* Petroluem Technology Development Fund Scholarship For Academic Excellence (Top 300 Student Nationwide)\n* Total E&P Scholarship (For Outstanding Students in the Country)\n* Nigerian Agip oil Company Scholarship Award (For Top Performing Student per State)\n\nEXPERIENCE\n\n* Alytics (January 2024 – Present)\n\t+ Senior Data Associate (Manchester, United Kingdom)\n\t+ Spearheaded tailored training sessions, empowering individuals and organizations with essential data management skills, including Excel data cleaning, dashboarding, and statistical analysis.\n\t+ Explored emerging technologies such as Large Language Models (LLMs), Computer Vision (CV), and Natural Language Processing (NLP) in research and product design, implementing innovative solutions across diverse domains.\n\t+ Facilitated engaging training experiences, fostering a culture of continuous learning and growth, and successfully training over 50,000 individuals in essential data science concepts and tools.\n* Amdari (September 2023 – February 2024)\n\t+ Subject Matter Expert, Data Science (Manchester, United Kingdom)\n\t+ Led the successful conclusion of multiple impactful case studies across energy, health, music, and finance sectors.\n\t+ Applied cutting-edge techniques like computer vision, large language models, and traditional machine learning to tackle diverse business challenges.\n\t+ Developed innovative solutions using a robust stack including RabbitMQ, Snowflake Data Warehouse, Flask, and Cloud Instances for seamless deployment.\n\t+ Completed 14 crucial projects in just 6 months, guiding each through experimentation, modeling, and deployment phases with precision.\n* Josplay Inc. (An African Music Intelligence Startup) (June 2022 – August 2023)\n\t+ Junior Machine Learning Engineer (Lagos, Lagos)\n\t+ Spearheaded the successful development and deployment of cutting-edge deep learning models for audio analysis, instrument detection, song genre classification, and music recommendation.\n\t+ Utilized state-of-the-art tools and frameworks such as TensorFlow and Pytorch for development and containerization technologies like Docker to deliver exceptional performance and accuracy.\n\t+ Pioneered the use of advanced audio signal processing libraries such as Librosa and Essentia to create robust and scalable pipelines for audio processing, analysis, and synthesis.\n\t+ Engineered and implemented high-performance, fault-tolerant systems that have enabled the real-time analysis of over 1,000,000 songs, delivering fast and accurate results at scale.\n\t+ Leveraged the power of leading cloud service providers like Amazon Web Services (AWS) and Google Cloud Platform (GCP), as well as technologies such as Pymongo (MongoDB), Flask, and Kafka, to build robust and reliable pipelines for audio processing and analysis.\n* National Information Technology Development Agency (NITHUB) (April 2021 – April 2022)\n\t+ Data Science Research Team Lead (Lagos, Lagos)\n\t+ Led a team of Data Scientists in conducting groundbreaking research aimed at improving traffic management systems through the application of advanced computer vision techniques.\n\t+ Developed state-of-the-art models with exceptional accuracy for real-time traffic pattern detection and analysis, leveraging powerful tools such as TensorFlow, Keras, and a pretrained YOLO v5 model.\n\t+ Developed a cutting-edge traffic lighting system that dynamically adapts to traffic patterns, resulting in a significant 15% reduction in wait time for motorists.\n* University of Lagos (November 2020 – May 2021)\n\t+ Data Research Assistant (Lagos, Lagos)\n\t+ Assisted in analyzing product quality data and detecting anomalies in product quality using Excel.\n\t+ Collaborated with team members to improve faulty product detection rate by 27.5%.\n\nPROJECTS\n\n* RAG-LLM System For Ingesting and Querying Companys’ Sustainability reports Using LLAMA And Langchain\n\t+ Utilized Langchain and Open Source, Large Language Model: LLAMA 7B, to create a pipeline for the ingestion, tokenization, and querying of information available in Company’s Sustainability Report.\n\nRESEARCH\n\n* APPLICATIONS OF ARTIFICIAL NEURAL NETWORKS IN MODELLING THE DE-\nHYDRATION PROPERTIES OF YAM(DIOSCOREA ROTUNDA)|Publication in Review\n\t+ Authors: Dr. AA Akinola (Google Scholar), Ibrahim Ibrahim, Obafemi Dolapo\n\nTECHNICAL SKILLS\n\n* Languages: Python, Java, Solidity, HTML/CSS, JavaScript, SQL, Matlab\n* Developer Tools: VS Code, AWS CLI, Google Colaboratory, Google Cloud Platform\n* Technologies/Frameworks: Linux, Tensorflow, Pytorch, Scikit-Learn, Pandas, Numpy, Seaborn, Pyplot, Hyperopt, Kafka, RabbitMQ, SnowFlakes, Langchain, Vader\n\nLEADERSHIP/EXTRACURRICULAR\n\n* Google Student Developer’s Club Open Hack Week 2020 (July 2020)\n\t+ Team Lead (Lagos, Lagos)\n\t+ Successfully led a team of 5 in developing a groundbreaking online platform that enables the exchange of recyclable waste for cash, with the ultimate goal of building a zero waste world. Conducted a thorough viability study of the product and crafted a highly effective pitch deck that propelled us to victory in the finals of the challenge.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#####WORK TO BE DONE#######\n#WE NEED A BETTER LLM ==\"> CHECK OUT GPT 4 AND OTHERS\"\n\n#WE NEED TO FIGURE OUT A WAY TO OUTPUT THE RESULT STRING AS A PDF FILE FORMATTED \n    #CORRECTLY ==> GPT could have an option of parsing the pdf either automatically or progmatically","metadata":{},"execution_count":null,"outputs":[]}]}