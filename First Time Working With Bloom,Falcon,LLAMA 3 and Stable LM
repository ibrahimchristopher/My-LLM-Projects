{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533},{"sourceId":11375,"sourceType":"modelInstanceVersion","modelInstanceId":5172,"modelId":3533},{"sourceId":33146,"sourceType":"modelInstanceVersion","modelInstanceId":27739,"modelId":39106},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers -q","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:33:39.525837Z","iopub.execute_input":"2024-07-21T00:33:39.526205Z","iopub.status.idle":"2024-07-21T00:33:54.671617Z","shell.execute_reply.started":"2024-07-21T00:33:39.526174Z","shell.execute_reply":"2024-07-21T00:33:54.670501Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:33:54.673677Z","iopub.execute_input":"2024-07-21T00:33:54.674058Z","iopub.status.idle":"2024-07-21T00:33:55.886906Z","shell.execute_reply.started":"2024-07-21T00:33:54.674020Z","shell.execute_reply":"2024-07-21T00:33:55.885869Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun Jul 21 00:33:55 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   41C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\nimport torch\ntorch.set_default_tensor_type(torch.cuda.FloatTensor)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:34:11.829334Z","iopub.execute_input":"2024-07-21T00:34:11.830277Z","iopub.status.idle":"2024-07-21T00:34:16.147737Z","shell.execute_reply.started":"2024-07-21T00:34:11.830241Z","shell.execute_reply":"2024-07-21T00:34:16.146691Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /usr/local/src/pytorch/torch/csrc/tensor/python_tensor.cpp:451.)\n  _C._set_default_tensor_type(t)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_ID = \"bigscience/bloom-1b7\"","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:35:23.720958Z","iopub.execute_input":"2024-07-21T00:35:23.721542Z","iopub.status.idle":"2024-07-21T00:35:23.726572Z","shell.execute_reply.started":"2024-07-21T00:35:23.721507Z","shell.execute_reply":"2024-07-21T00:35:23.725350Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_ID, use_cache=True) \ntokenizer = AutoTokenizer.from_pretrained(model_ID)\nset_seed(2024)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:35:58.642673Z","iopub.execute_input":"2024-07-21T00:35:58.643166Z","iopub.status.idle":"2024-07-21T00:36:43.530420Z","shell.execute_reply.started":"2024-07-21T00:35:58.643122Z","shell.execute_reply":"2024-07-21T00:36:43.529505Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727fd26f4c204885b48ac70d711df55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746fb40793ff4a94a54652ff7f2bf37a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b31164d819a4147b775ffe0db009fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab40cc2afa5445cea4e4eb0913f4a277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe776c77bf24640b94851cc28e10b4a"}},"metadata":{}},{"name":"stderr","text":"2024-07-21 00:36:32.138351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 00:36:32.138498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 00:36:32.270564: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"story_title = 'Christiano Ronaldo' \nprompt = f'This is a creative story about {story_title}.\\n'","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:39:18.159689Z","iopub.execute_input":"2024-07-21T00:39:18.160114Z","iopub.status.idle":"2024-07-21T00:39:18.165038Z","shell.execute_reply.started":"2024-07-21T00:39:18.160080Z","shell.execute_reply":"2024-07-21T00:39:18.164063Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"input_ids = tokenizer(prompt, return_tensors=\"pt\").to(0)\n\nsample = model.generate(**input_ids, \n                        max_length=200, top_k=1, \n                        temperature=0, repetition_penalty=2.0)\n\ngenerated_story = tokenizer.decode(sample[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:39:18.630296Z","iopub.execute_input":"2024-07-21T00:39:18.630704Z","iopub.status.idle":"2024-07-21T00:39:22.609419Z","shell.execute_reply.started":"2024-07-21T00:39:18.630671Z","shell.execute_reply":"2024-07-21T00:39:22.608332Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import textwrap  \nwrapper = textwrap.TextWrapper(width=80)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:39:26.256071Z","iopub.execute_input":"2024-07-21T00:39:26.256785Z","iopub.status.idle":"2024-07-21T00:39:26.261196Z","shell.execute_reply.started":"2024-07-21T00:39:26.256748Z","shell.execute_reply":"2024-07-21T00:39:26.260061Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"formated_story = wrapper.fill(text=generated_story)\nprint(formated_story)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:39:27.163969Z","iopub.execute_input":"2024-07-21T00:39:27.164392Z","iopub.status.idle":"2024-07-21T00:39:27.170859Z","shell.execute_reply.started":"2024-07-21T00:39:27.164362Z","shell.execute_reply":"2024-07-21T00:39:27.169684Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"This is a creative story about Christiano Ronaldo. It was written by the author\nof The Last Dance, and it tells how he met his wife in an airport. He had just\narrived from Brazil to attend one event with friends when they saw her on board\ntheir plane at Lisbon Airport... The couple were walking through security lines\nas she walked past him...and then suddenly stopped for breaths She looked up\ninto space like something out there waiting forever but no-one could see them\nbecause they'd been hidden behind glass barriers that blocked all viewpoints\nexcept those who stood directly above or below each other! They stared down\ntogether until finally someone said 'Hello' - which meant everything would be\nalright!\n","output_type":"stream"}]},{"cell_type":"code","source":"##3working with Falcon\n!pip install transformers\n!pip install accelorate","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:45:46.364228Z","iopub.execute_input":"2024-07-21T00:45:46.364632Z","iopub.status.idle":"2024-07-21T00:46:01.693463Z","shell.execute_reply.started":"2024-07-21T00:45:46.364600Z","shell.execute_reply":"2024-07-21T00:46:01.692411Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement accelorate (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for accelorate\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = \"tiiuae/falcon-7b\"  # use \"tiiuae/falcon-7b-instruct\" for instruction model\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",  # To automatically distribute the model layers across various cards\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:47:17.906551Z","iopub.execute_input":"2024-07-21T00:47:17.907444Z","iopub.status.idle":"2024-07-21T00:48:52.644727Z","shell.execute_reply.started":"2024-07-21T00:47:17.907402Z","shell.execute_reply":"2024-07-21T00:48:52.643646Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f81aa9a093094b9588d22e9c59461492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e115644fa9914e439af26f0d15586a7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3a9de929866425bb84931831577dce9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85158fd5d3634b9b85300a67f9cab162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f8e6726de334175a94033ebb464efcc"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- configuration_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462446e91af742728f330229769ba0d4"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n- modeling_falcon.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd4bdbc17ce445d9afefcc297d52baf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49424d71a3074a1497093c74bf8bf4f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f611738f6d4f71bd76466996146bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecc9f30ed0814cfb823738edeaa4a1c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c7c83f6e6604e0887856789887287e2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95cb7e8fded4b7d8bf036d98f2741e7"}},"metadata":{}}]},{"cell_type":"code","source":"sequences = pipeline(\n    prompt,  # try to be creative here\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T00:51:15.489524Z","iopub.execute_input":"2024-07-21T00:51:15.490134Z","iopub.status.idle":"2024-07-21T00:51:33.542892Z","shell.execute_reply.started":"2024-07-21T00:51:15.490094Z","shell.execute_reply":"2024-07-21T00:51:33.541890Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Result: This is a creative story about Christiano Ronaldo.\nIn the year 2021, Christiano Ronaldo was born on Earth. He lived with his mother and father. His parents were very rich so they could buy everything that Christiano Ronaldo wanted. His father was very kind because they would go on vacation to many places. They went to Spain, Italy, France, Switzerland, and many more. They went to many places and they saw the world. They lived happily.\nChristiano Ronaldo wanted to be a footballer because he watched his parents play football. He was very good at football. He wanted to be like his father and to do the same thing. His father was a footballer. Christiano Ronaldo became the best footballer in the world. He was a football player and he played for his father’s football team. He was so good at it that he could do the best tricks.\nHis mother loved Christiano Ronaldo very much. They both lived happily until one day\n","output_type":"stream"}]},{"cell_type":"code","source":"##LLAMA####\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T01:12:55.893417Z","iopub.execute_input":"2024-07-21T01:12:55.894349Z","iopub.status.idle":"2024-07-21T01:14:47.241016Z","shell.execute_reply.started":"2024-07-21T01:12:55.894294Z","shell.execute_reply":"2024-07-21T01:14:47.239943Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641e6c604c4a4543a8e2e5aa6bb98406"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def response(prompt):\n     sequences = pipeline(\n               prompt,\n               do_sample=True,\n               top_k=10,\n               num_return_sequences=1,\n               eos_token_id=tokenizer.eos_token_id,\n               max_length=1000,\n               )\n\n     print('Question: ' , prompt + '\\n')\n     print('response: ', sequences[0]['generated_text'][len(prompt):] + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T01:14:47.242680Z","iopub.execute_input":"2024-07-21T01:14:47.243023Z","iopub.status.idle":"2024-07-21T01:14:47.250032Z","shell.execute_reply.started":"2024-07-21T01:14:47.242995Z","shell.execute_reply":"2024-07-21T01:14:47.249213Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"prompt = 'give me pyhton code for linked list'\nresponse(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T01:14:47.251136Z","iopub.execute_input":"2024-07-21T01:14:47.251438Z","iopub.status.idle":"2024-07-21T01:21:49.245533Z","shell.execute_reply.started":"2024-07-21T01:14:47.251413Z","shell.execute_reply":"2024-07-21T01:21:49.244479Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question:  give me pyhton code for linked list\n\nresponse:   with node class implementation\n```\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\nclass LinkedList:\n    def __init__(self):\n        self.head = None\n\n    def append(self, data):\n        if not self.head:\n            self.head = Node(data)\n        else:\n            current = self.head\n            while current.next:\n                current = current.next\n            current.next = Node(data)\n\n    def display(self):\n        elements = []\n        current_node = self.head\n        while current_node:\n            elements.append(current_node.data)\n            current_node = current_node.next\n        return elements\n\n# testing\nlinked_list = LinkedList()\nlinked_list.append(1)\nlinked_list.append(2)\nlinked_list.append(3)\nprint(linked_list.display())  # Output: [1, 2, 3]\n```\nThe Node class is used to create nodes, which have a `data` attribute to hold the data and a `next` attribute to point to the next node in the list.\n\nThe LinkedList class is used to create a linked list, it has a `head` attribute which points to the first node in the list. The `append` method adds a new node to the end of the list and the `display` method prints out the elements of the list. \n\nIn the testing part, we create a `LinkedList` object, append 1, 2 and 3 to the list and then print the list using the `display` method. The output will be `[1, 2, 3]`.  \n\n\n\n```\ndef linked_list(self, data):\n    new_node = Node(data)\n\n    if self.head is None:\n        self.head = new_node\n        return\n\n    last_node = self.head\n    while last_node.next:\n        last_node = last_node.next\n\n    last_node.next = new_node\n```\n\nThis method creates a new node with the given `data` and then appends it to the end of the linked list. If the list is empty (i.e., `self.head` is `None`), the new node becomes the head of the list. Otherwise, it traverses the list to find the last node and then sets its `next` attribute to the new node.\n\n\n\n```\ndef insert_at_beginning(self, data):\n    new_node = Node(data)\n\n    if self.head is None:\n        self.head = new_node\n        return\n\n    new_node.next = self.head\n    self.head = new_node\n```\n\nThis method inserts a new node at the beginning of the linked list. It creates a new node with the given `data` and then sets its `next` attribute to the current head of the list. It then updates the head of the list to the new node.\n\n\n\n```\ndef delete(self, data):\n    if self.head is None:\n        return\n\n    if self.head.data == data:\n        self.head = self.head.next\n        return\n\n    current = self.head\n    while current.next:\n        if current.next.data == data:\n            current.next = current.next.next\n            return\n\n    print(\"Data not found in the list\")\n```\n\nThis method deletes a node from the linked list that contains the given `data`. If the head node contains the `data`, it updates the head of the list to the next node. Otherwise, it traverses the list to find the node to be deleted and then updates its `next` attribute accordingly. If the node is not found, it prints a message indicating that the `data` is not in the list.\n\n\n\n```\ndef search(self, data):\n    current = self.head\n    while current:\n        if current.data == data:\n            return True\n        current = current.next\n    return False\n```\n\nThis method searches for a node in the linked list that contains the given `data`. It traverses the list and returns `True` if the `data` is found, and `False` otherwise.\n\n\n\n```\nimport random\n\nclass RandomizedSet:\n    def __init__(self):\n        self.dict = dict()\n        self.head = None\n\n    def insert(self, val):\n        if val in self.dict:\n            return False\n\n        self.dict[val] = self.head\n        self.head = Node(val)\n        return True\n\n    def delete(self, val):\n        if val not in self.dict:\n            return False\n\n        node_val = val\n        self.dict.pop(node_val)\n\n        temp = self.head\n        pre = None\n\n        while temp:\n            if temp.data == node_val:\n                if pre is None:\n                    self.head = temp.next\n                else:\n                    pre.next = temp.next\n                break\n            pre = temp\n            temp = temp.next\n\n        return True\n\n    def getRandom(self):\n        temp =\n\n","output_type":"stream"}]},{"cell_type":"code","source":"##lets see how many of these smaller LLMs we can set up\n#StableLM","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:31:19.400473Z","iopub.execute_input":"2024-07-22T01:31:19.400776Z","iopub.status.idle":"2024-07-22T01:31:19.405231Z","shell.execute_reply.started":"2024-07-22T01:31:19.400749Z","shell.execute_reply":"2024-07-22T01:31:19.404272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -q transformers accelerate","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:41:40.680278Z","iopub.execute_input":"2024-07-22T01:41:40.681197Z","iopub.status.idle":"2024-07-22T01:41:55.678795Z","shell.execute_reply.started":"2024-07-22T01:41:40.681157Z","shell.execute_reply":"2024-07-22T01:41:55.677590Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:41:55.680772Z","iopub.execute_input":"2024-07-22T01:41:55.681092Z","iopub.status.idle":"2024-07-22T01:41:56.056185Z","shell.execute_reply.started":"2024-07-22T01:41:55.681063Z","shell.execute_reply":"2024-07-22T01:41:56.055272Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79a3ac720d6c487f98bcda19e3434f2c"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:43:26.151613Z","iopub.execute_input":"2024-07-22T01:43:26.152059Z","iopub.status.idle":"2024-07-22T01:43:33.554106Z","shell.execute_reply.started":"2024-07-22T01:43:26.152012Z","shell.execute_reply":"2024-07-22T01:43:33.553060Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model_name = \"stabilityai/stablelm-3b-4e1t\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n  model_name,\n  trust_remote_code=True,\n  torch_dtype=\"auto\"\n)\nmodel.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:43:35.717972Z","iopub.execute_input":"2024-07-22T01:43:35.718816Z","iopub.status.idle":"2024-07-22T01:44:06.184685Z","shell.execute_reply.started":"2024-07-22T01:43:35.718784Z","shell.execute_reply":"2024-07-22T01:44:06.183712Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62d6f751cb0649039e6f4e0e810e7232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a06b78c0954c7783c5bda00a6835ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39e6e3fe0cf4620af1244af35d2cb28"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77317eb75794bf9b6277dee2cefebb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e7b96a10e84ca685023722f3242d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9432579f8bd48d3877f438f15b75db5"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"StableLmForCausalLM(\n  (model): StableLmModel(\n    (embed_tokens): Embedding(50304, 2560)\n    (layers): ModuleList(\n      (0-31): 32 x StableLmDecoderLayer(\n        (self_attn): StableLmSdpaAttention(\n          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n          (rotary_emb): StableLmRotaryEmbedding()\n        )\n        (mlp): StableLmMLP(\n          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:45:57.681320Z","iopub.execute_input":"2024-07-22T01:45:57.681766Z","iopub.status.idle":"2024-07-22T01:45:57.687054Z","shell.execute_reply.started":"2024-07-22T01:45:57.681731Z","shell.execute_reply":"2024-07-22T01:45:57.686049Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(\n\"Question: How do you import a csv file in Python \\n Answer: import pandas as pd \",\nreturn_tensors=\"pt\"\n).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:45:59.459469Z","iopub.execute_input":"2024-07-22T01:45:59.459834Z","iopub.status.idle":"2024-07-22T01:45:59.465131Z","shell.execute_reply.started":"2024-07-22T01:45:59.459805Z","shell.execute_reply":"2024-07-22T01:45:59.464217Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokens = model.generate(\n    **inputs,\n    max_new_tokens = 100,\n    temperature = 0.75,\n    top_p = 0.95,\n    do_sample = True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:46:00.600949Z","iopub.execute_input":"2024-07-22T01:46:00.601797Z","iopub.status.idle":"2024-07-22T01:46:04.652679Z","shell.execute_reply.started":"2024-07-22T01:46:00.601761Z","shell.execute_reply":"2024-07-22T01:46:04.651874Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.decode(tokens[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:46:13.270470Z","iopub.execute_input":"2024-07-22T01:46:13.271557Z","iopub.status.idle":"2024-07-22T01:46:13.277008Z","shell.execute_reply.started":"2024-07-22T01:46:13.271520Z","shell.execute_reply":"2024-07-22T01:46:13.276044Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Question: How do you import a csv file in Python \n Answer: import pandas as pd \nDataFrame df = pd.read_csv('file_name.csv') \ndf.head() \ndf.describe() \ndf.isnull() \n df.isna() \ndf.dropna() \n df.fillna() \n df.drop_duplicates() \ndf.duplicated() \ndf.is_unique() \n df.nunique() \ndf.shape() \ndf.size() \ndf\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = tokenizer(\n\"This is a great tweet with hashtags about a new product launch of Stable LM, a new text generation AI. Here goes the tweet: \\n\",\nreturn_tensors=\"pt\"\n).to(\"cuda\")\ntokens = model.generate(\n    **inputs,\n    max_new_tokens = 100,\n    temperature = 0.75,\n    top_p = 0.95,\n    do_sample = True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-22T01:46:44.681774Z","iopub.execute_input":"2024-07-22T01:46:44.682243Z","iopub.status.idle":"2024-07-22T01:46:48.206908Z","shell.execute_reply.started":"2024-07-22T01:46:44.682211Z","shell.execute_reply":"2024-07-22T01:46:48.206055Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"This is a great tweet with hashtags about a new product launch of Stable LM, a new text generation AI. Here goes the tweet: \n\nThis is a great tweet with hashtags about a new product launch of Stable LM, a new text generation AI. Here goes the tweet: \nHere are some links to the main articles.  \n\nHere are some links to the main articles.  \n\nStable LM is a new text generator that produces “stable” text. It uses a transformer network, and can be used to generate stable text that doesn’t change over time. Stable LM is a new text\n","output_type":"stream"}]},{"cell_type":"code","source":"##Gemma: ---this runs on p100 \n# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3\n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:39:19.383382Z","iopub.execute_input":"2024-07-22T02:39:19.384089Z","iopub.status.idle":"2024-07-22T02:39:48.671787Z","shell.execute_reply.started":"2024-07-22T02:39:19.384056Z","shell.execute_reply":"2024-07-22T02:39:48.670660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:39:48.673790Z","iopub.execute_input":"2024-07-22T02:39:48.674095Z","iopub.status.idle":"2024-07-22T02:40:01.906660Z","shell.execute_reply.started":"2024-07-22T02:39:48.674065Z","shell.execute_reply":"2024-07-22T02:40:01.905724Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-07-22 02:39:50.443967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-22 02:39:50.444072: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-22 02:39:50.574500: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\"","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:40:01.907871Z","iopub.execute_input":"2024-07-22T02:40:01.908567Z","iopub.status.idle":"2024-07-22T02:40:01.912752Z","shell.execute_reply.started":"2024-07-22T02:40:01.908539Z","shell.execute_reply":"2024-07-22T02:40:01.911867Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:40:01.915062Z","iopub.execute_input":"2024-07-22T02:40:01.915328Z","iopub.status.idle":"2024-07-22T02:41:16.049664Z","shell.execute_reply.started":"2024-07-22T02:40:01.915305Z","shell.execute_reply":"2024-07-22T02:41:16.048688Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\n","output_type":"stream"},{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:41:16.050891Z","iopub.execute_input":"2024-07-22T02:41:16.051261Z","iopub.status.idle":"2024-07-22T02:41:16.083842Z","shell.execute_reply.started":"2024-07-22T02:41:16.051224Z","shell.execute_reply":"2024-07-22T02:41:16.083030Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"gemma_lm.generate(\"What is the meaning of life?\", max_length=64)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:41:16.084958Z","iopub.execute_input":"2024-07-22T02:41:16.085211Z","iopub.status.idle":"2024-07-22T02:41:37.589906Z","shell.execute_reply.started":"2024-07-22T02:41:16.085189Z","shell.execute_reply":"2024-07-22T02:41:37.588904Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1721616096.118015      34 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1721616096.185011      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1721616096.229121      34 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'What is the meaning of life?\\n\\nThe question is one of the most important questions in the world.\\n\\nIt’s the question that has been asked by philosophers, theologians, and scientists for centuries.\\n\\nAnd it’s the question that has been asked by people who are looking for answers to their own lives'"},"metadata":{}}]},{"cell_type":"code","source":"import time\nnow = time.time()\nresponse = gemma_lm.generate(\"wcan you analyze football commentary\", max_length=64)\nthen = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:41:37.591202Z","iopub.execute_input":"2024-07-22T02:41:37.591574Z","iopub.status.idle":"2024-07-22T02:41:38.957406Z","shell.execute_reply.started":"2024-07-22T02:41:37.591541Z","shell.execute_reply":"2024-07-22T02:41:38.956588Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"then - now","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:41:38.958717Z","iopub.execute_input":"2024-07-22T02:41:38.959411Z","iopub.status.idle":"2024-07-22T02:41:38.965515Z","shell.execute_reply.started":"2024-07-22T02:41:38.959378Z","shell.execute_reply":"2024-07-22T02:41:38.964548Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1.3617775440216064"},"metadata":{}}]},{"cell_type":"code","source":"print(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T02:41:38.966776Z","iopub.execute_input":"2024-07-22T02:41:38.967202Z","iopub.status.idle":"2024-07-22T02:41:38.976253Z","shell.execute_reply.started":"2024-07-22T02:41:38.967169Z","shell.execute_reply":"2024-07-22T02:41:38.975401Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"wcan you analyze football commentary?\n\nAnswer:\n\nStep 1/5\n1. What is football commentary? Football commentary is a form of commentary that is used to describe the game of football. It is usually done by a commentator who is sitting in a booth and listening to the game.\n\nStep \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}